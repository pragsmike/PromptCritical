# PromptCritical: Evolving prompts in a closed category

*A reproducible platform for evolving largeâ€“languageâ€“model prompts, one small,
auditable step at a time.*

## âœ¨ What is PromptCritical?

PromptCritical is a **dataâ€‘driven, experimentâ€‘oriented toolchain** that breeds and evaluates prompts for LLMs. It automates the cycle of:

```
bootstrap â†’ contest (Failter) â†’ record â†’ evolve
```

so you can focus on defining **fitness metrics** and **mutation strategies**, not on plumbing.

Key ingredients:

| Ingredient | Purpose |
| :--- | :--- |
| **Polylith workspace** | Reâ€‘usable components, clean boundaries, lightningâ€‘fast incremental tests |
| **Immutable Prompt DB** | Atomic, hashâ€‘verified store with perâ€‘file lockâ€‘healing |
| **Failter integration** | Runs largeâ€‘scale prompt contests and collects scores |
| **Evolution engine** (*WIP*) | Selects, mutates & crossâ€‘breeds prompts toward higher fitness |

---

## âœ¨ Aspirational Goals

Prompt engineering still feels like folklore. PromptCriticalâ€™s long-term mission is to turn it into a **data-driven, evolutionary workflow**:

1.  **Store every prompt immutably** with lineage, hashes, and timestamps.
2.  **Run controlled experiments** that score those prompts on real tasks (Latency / Cost / Accuracy / Consistency).
3.  **Breed the next generation**â€”mutate, crossover, and selectâ€”using the recorded scores as fitness.
4.  **Repeat automatically**, producing prompts that keep pace with new LLM releases and changing task definitions.

---

## ğŸ— Workspace Layout (Polylith)

The project follows Polylith conventions, organizing the codebase into re-usable **components** and runnable **bases**. This ensures logic is reusable by any interface (e.g., the CLI or a future web service).

```
workspace/
â”œâ”€â”€ components/
â”‚   â”œâ”€â”€ command/      ; Reusable, high-level user commands
â”‚   â”œâ”€â”€ expdir/       ; Manages experiment directory layout
â”‚   â”œâ”€â”€ pdb/          ; The immutable prompt database
â”‚   â”œâ”€â”€ pop/          ; Population domain model & analysis
â”‚   â”œâ”€â”€ config/       ; Runtime configuration (EDN â†’ map)
â”‚   â”œâ”€â”€ log/          ; Structured logging facade
â”‚   â”œâ”€â”€ llm/          ; Thin HTTP client for LLMs
â”‚   â””â”€â”€ test-helper/  ; Shared utilities for testing
â””â”€â”€ bases/
    â””â”€â”€ cli/          ; `pcrit` commandâ€‘line entry point
```

### CLI Overview

| Command | Status | Description |
| :--- | :--- | :--- |
| `bootstrap` | âœ… | Seed the Prompt DB from a manifest |
| `contest` | ğŸ”œ | Package prompts, run Failter, collect report |
| `record` | ğŸ”œ | Record and analyze scores from `report.csv` |
| `evolve` | ğŸ”œ | Generate next generation via mutation/crossover |

---

## ğŸ“¦ Current State (Post-Refactoring)

The initial v0.1 work is complete, and the project has undergone a significant architectural refactoring. The codebase is now organized into a clean Polylith structure with clear, single-responsibility components.

*   **`pcrit.command`**: Provides reusable, high-level workflow functions (e.g., `bootstrap!`) that can be called by any base.
*   **`pcrit.expdir`**: Manages the physical filesystem layout of an experiment directory.
*   **`pcrit.pdb`**: The robust, concurrent, and immutable prompt database.
*   **`pcrit.pop`**: Handles core prompt domain logic, including ingestion and analysis (e.g., assigning a `:prompt-type` to every prompt).

The `bootstrap` command is fully implemented according to this improved architecture.

---

## ğŸ›  External Dependency â€” Failter

PromptCritical does **not** implement scoring or judgement itself. Instead we treat [**Failter**](https://github.com/pragsmike/failter) as a **black box** experiment runner:

*   We build a directory that matches Failterâ€™s required structure (`inputs/`, `templates/`, `model-names.txt`, â€¦).
*   We shell-out to `failter experiment â†’ evaluate â†’ report`.
*   We parse the resulting `report.csv` to record fitness data for the evolution loop.

---

## ğŸš§ Current Milestone (v0.2): The Vertical Slice

The immediate goal is a **â€œbootstrap â†’ contest â†’ recordâ€ vertical slice**. This will prove the system can orchestrate an external evaluator and round-trip the results.

1.  **Bootstrap an Experiment** (`âœ… Implemented`)
    Creates an initial population from a manifest file.
    ```bash
    pcrit bootstrap my-experiment/
    ```

2.  **Run a Contest** (`ğŸ”œ In Development`)
    Packages prompts, runs them through Failter, and collects the results.
    ```bash
    pcrit contest my-experiment/ --prompts P1,P2 --inputs ...
    ```

3.  **Record the Scores** (`ğŸ”œ In Development`)
    Parses the `report.csv` from Failter and records the fitness data in the experiment's history, linking scores back to the prompts that earned them.

---

## ğŸ—º Roadmap Snapshot

| Milestone | New Capability |
| :--- | :--- |
| **v0.2** | Bootstrap â†’ Contest â†’ Record (described above) |
| **v0.3** | Basic mutation & crossover operators |
| **v0.4** | Simple (Âµ+Î») evolutionary loop driven by contest scores |
| **v0.5** | Surrogate critic to pre-filter variants before Failter |
| **v0.6** | Experiment recipes (EDN/YAML) and CLI replayability |
| **v0.7** | Reporting dashboard (`pcrit.web` base) |
| **v1.0** | Distributed workers, advanced semantic validators |

---

## Getting Involved

1.  **Clone** and run the tests:
    ```bash
    git clone https://github.com/pragsmike/promptcritical
    cd promptcritical
    make test
    ```
2.  **Read** the design documents in the `docs/` directory.
3.  **Hack** on the next milestoneâ€”PRs welcome!

---

**PromptCritical**: because great prompts shouldnâ€™t be accidental.
